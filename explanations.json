{
  "algorithms": {
    "q-learning": {
      "title": "Q-Learning (Off-Policy TD)",
      "text": "<p><strong>Goal:</strong> Learns the optimal action-value function \\(Q^*(s,a)\\), which represents the maximum expected future reward for taking action \\(a\\) in state \\(s\\) and following the optimal policy thereafter.</p><p><strong>How it works:</strong> It updates the Q-value for the current state-action pair based on the reward received and the <em>maximum</em> Q-value of the <em>next</em> state (greedy approach). It's \"off-policy\" because the action used for the update (the best next action) might be different from the action the agent actually takes next (which could be exploratory).</p><p><strong>Update Rule:</strong></p>\\[ Q(s,a) \\leftarrow Q(s,a) + \\alpha [R + \\gamma \\max_{a'} Q(s',a') - Q(s,a)] \\]<p>Where \\(\\alpha\\) is the <strong>Learning Rate</strong> (controls how much new information overrides the old estimate, typically 0 < \\(\\alpha\\) ≤ 1) and \\(\\gamma\\) is the <strong>Discount Factor</strong> (determines the importance of future rewards; 0 means only immediate reward matters, close to 1 means future rewards are highly valued, typically 0 ≤ \\(\\gamma\\) < 1).</p>"
    },
    "sarsa": {
      "title": "SARSA (On-Policy TD)",
      "text": "<p><strong>Goal:</strong> Learns the action-value function \\(Q(s,a)\\) for the <em>current policy</em> being followed by the agent (including exploration).</p><p><strong>How it works:</strong> It updates the Q-value based on the reward received and the Q-value of the <em>actual</em> state-action pair \\((s', a')\\) that comes next according to the current policy. It's \"on-policy\" because the update uses the same policy that generated the action.</p><p><strong>Update Rule:</strong></p>\\[ Q(s,a) \\leftarrow Q(s,a) + \\alpha [R + \\gamma Q(s',a') - Q(s,a)] \\]<p>Where \\(a'\\) is the action actually taken in state \\(s'\\). \\(\\alpha\\) is the <strong>Learning Rate</strong> (step size for updates) and \\(\\gamma\\) is the <strong>Discount Factor</strong> (importance of future rewards). SARSA stands for State-Action-Reward-State-Action.</p>"
    },
    "expected-sarsa": {
      "title": "Expected SARSA (On-Policy TD)",
      "text": "<p><strong>Goal:</strong> Learns the action-value function \\(Q(s,a)\\) for the <em>current policy</em>, similar to SARSA, but often with lower variance.</p><p><strong>How it works:</strong> Instead of using the Q-value of the single next action taken \\((a')\\), it uses the <em>expected</em> Q-value of the next state. This expectation is calculated by averaging the Q-values of all possible next actions, weighted by their probabilities \\(\\pi(a'|s')\\) under the current policy.</p><p><strong>Update Rule:</strong></p>\\[ Q(s,a) \\leftarrow Q(s,a) + \\alpha \\left[ R + \\gamma \\sum_{a'} \\pi(a'|s') Q(s',a') - Q(s,a) \\right] \\]<p>Where \\(\\alpha\\) is the <strong>Learning Rate</strong> (step size for updates) and \\(\\gamma\\) is the <strong>Discount Factor</strong> (importance of future rewards). This avoids the randomness introduced by sampling \\(a'\\) in the update, leading to smoother learning.</p>"
    },
    "monte-carlo": {
      "title": "Monte Carlo (MC) Control",
      "text": "<p><strong>Goal:</strong> Learns the action-value function \\(Q(s,a)\\) by averaging the returns (total discounted rewards) observed after visiting state-action pairs over many complete episodes.</p><p><strong>How it works:</strong> The agent runs through an entire episode. Only after the episode is finished does it go back through the steps taken. For each state-action pair \\((s,a)\\) visited at timestep \\(t\\), it calculates the total discounted reward \\(G_t = R_{t+1} + \\gamma R_{t+2} + \\dots\\) received from that point until the end of the episode. It then updates \\(Q(s,a)\\) to be closer to the average \\(G_t\\) observed for that pair.</p><p><strong>Update Rule (Simplified, Every-Visit):</strong></p>\\[ Q(s_t,a_t) \\leftarrow Q(s_t,a_t) + \\alpha [G_t - Q(s_t,a_t)] \\]<p>Where \\(\\alpha\\) is the <strong>Learning Rate</strong>, controlling how much the latest episode's return updates the average, and \\(\\gamma\\) (the <strong>Discount Factor</strong> used in \\(G_t\\)) determines the present value of future rewards. MC methods don't require a model of the environment and learn directly from experience, but they can only be applied to episodic tasks and updates only happen at the end of episodes.</p>"
    },
    "actor-critic": {
      "title": "Actor-Critic (Advantage Actor-Critic)",
      "text": "<p><strong>Goal:</strong> Combines policy learning (Actor) and value learning (Critic) to find an optimal policy.</p><p><strong>How it works:</strong><ul><li>The <strong>Critic</strong> learns a state-value function \\(V(s)\\), estimating how good it is to be in state \\(s\\). It uses Temporal Difference (TD) learning.</li><li>The <strong>Actor</strong> learns a policy \\(\\pi(a|s)\\), represented by preferences \\(h(s,a)\\), which dictate the probability of taking action \\(a\\) in state \\(s\\) (usually via softmax).</li><li>The Critic calculates the <strong>TD Error</strong> (or Advantage): \\(\\delta_t = R_{t+1} + \\gamma V(s_{t+1}) - V(s_t)\\). Here, \\(\\gamma\\) is the <strong>Discount Factor</strong>, balancing immediate reward and discounted future value.</li><li>The Critic updates \\(V(s_t)\\) using \\(\\delta_t\\): \\(V(s_t) \\leftarrow V(s_t) + \\alpha_c \\delta_t\\). \\(\\alpha_c\\) is the critic's <strong>Learning Rate</strong>.</li><li>The Actor updates its preferences \\(h(s_t, a_t)\\) using \\(\\delta_t\\) and the policy gradient (often with its own learning rate \\(\\alpha_a\\)), essentially increasing the preference for actions that led to positive TD errors and decreasing it for those leading to negative errors.</li></ul></p><p>This allows the agent to learn online (like TD methods) while directly optimizing the policy (like policy gradient methods).</p>"
    },
    "sr": {
      "title": "Successor Representation (SR)",
      "text": "<p><strong>Goal:</strong> Learns a predictive representation of state occupancy (Successor Representation, \\(M\\)) and reward weights (\\(w\\)) to compute state values \\(V\\) and action values \\(Q\\). It decouples environment dynamics/transitions from rewards.</p><p><strong>How it works:</strong><ul><li>The <strong>Successor Representation \\(M(s, s')\\)</strong> learns the expected discounted future occupancy of state \\(s'\\) starting from state \\(s\\) following the current policy. It's learned via TD updates.</li><li>The <strong>Reward Weights \\(w(s')\\)</strong> learn the expected immediate reward received upon entering state \\(s'\\). Also learned via TD.</li><li>The <strong>State Value \\(V(s)\\)</strong> is computed by combining \\(M\\) and \\(w\\): \\( V(s) = \\sum_{s'} M(s, s') w(s') \\).</li><li>The <strong>Action Value \\(Q(s, a)\\)</strong> can be estimated based on the expected next state \\(s'\\) after taking action \\(a\\) in state \\(s\\): \\( Q(s, a) \\approx w(s') + \\gamma V(s') \\) (using the learned \\(w(s')\\) as the immediate reward estimate).</li><li>Action selection uses these computed Q-values with standard exploration strategies (ε-Greedy, Softmax).</li></ul></p><p><strong>Update Rules:</strong></p><p>1. Weight Update (after \\(s \\rightarrow s'\\) with reward \\(R\\)):</p>\\[ w(s') \\leftarrow w(s') + \\alpha_w [R - w(s')] \\]<p>2. SR Update (for all \\(s_{prime}\\) in the grid):</p>\\[ M(s, s_{prime}) \\leftarrow M(s, s_{prime}) + \\alpha_M [ \\mathbb{I}(s' = s_{prime}) + \\gamma M(s', s_{prime}) - M(s, s_{prime}) ] \\]<p>Where \\(\\mathbb{I}(\\cdot)\\) is the indicator function. \\(\\alpha_w\\) is the <strong>Learning Rate</strong> for reward weights, \\(\\alpha_M\\) is the <strong>Learning Rate</strong> for the SR matrix, and \\(\\gamma\\) is the <strong>Discount Factor</strong> controlling how much future state occupancies contribute to the current state's representation. Note that updating \\(M\\) requires iterating over all possible states \\(s_{prime}\\) in each step, making it computationally more intensive than standard TD methods.</p>"
    }
  },
  "strategies": {
    "epsilon-greedy": {
      "title": "ε-Greedy Exploration",
      "text": "<p><strong>Idea:</strong> Mostly act greedily (exploit), but sometimes explore randomly.</p><p><strong>How it works:</strong> With probability \\(1 - \\epsilon\\), choose the action with the highest estimated Q-value (\\(\\arg\\max_a Q(s,a)\\)). With probability \\(\\epsilon\\), choose a random action. The Exploration Rate \\(\\epsilon\\) controls the trade-off. Higher \\(\\epsilon\\) means more exploration.</p>"
    },
    "softmax": {
      "title": "Softmax (Boltzmann) Exploration",
      "text": "<p><strong>Idea:</strong> Choose actions probabilistically based on their Q-values. Higher Q-values mean higher probability.</p><p><strong>How it works:</strong> It calculates a probability for each action using the Boltzmann distribution:</p>\\[ P(a|s) = \\frac{\\exp(\\beta Q(s,a))}{\\sum_{a'} \\exp(\\beta Q(s,a'))} \\]<p>The Softmax Temperature parameter \\(\\beta\\) controls the sensitivity to Q-values. Higher \\(\\beta\\) leads to more deterministic (greedy) choices (probabilities concentrate on the max Q-value), while lower \\(\\beta\\) (closer to 0) leads to more uniform, random choices. A value of 0 would make all actions equally likely.</p><p>Unlike ε-Greedy, even non-best actions have a chance of being selected, proportional to their exponentiated Q-value.</p>"
    }
  }
} 